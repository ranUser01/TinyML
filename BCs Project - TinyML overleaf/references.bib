
@article{gemaque_overview_2020,
	title = {An overview of unsupervised drift detection methods},
	volume = {10},
	rights = {© 2020 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1381},
	doi = {10.1002/widm.1381},
	abstract = {Practical applications involving big data, such as weather monitoring, identification of customer preferences, Internet log analysis, and sensors warnings require challenging data analysis, since these are examples of problems whose data are generated in streams and usually demand real-time analytics. Patterns in such data stream problems may change quickly. Consequently, machine learning models that operate in this context must be updated over time. This phenomenon is called concept drift in machine learning and data mining literature. Several different directions have been pursued to learn from data stream and to deal with concept drift. However, most drift detection methods consider that an instance's class label is available right after its prediction, since these methods work by monitoring the prediction results of a base classifier or an ensemble of classifiers. Nevertheless, this constraint is unrealistic in several practical problems. To cope with this constraint, some works are focused on proposing efficient unsupervised or semi-supervised concept drift detectors. While interesting and recent overview papers dedicated to supervised drift detectors have been published, the scenario is not the same in terms of unsupervised methods. Therefore, this work presents a comprehensive overview of approaches that tackle concept drift in classification problems in an unsupervised manner. Additional contribution includes a proposed taxonomy of state-of-the-art approaches for concept drift detection based on unsupervised strategies. This article is categorized under: Technologies {\textgreater} Classification Technologies {\textgreater} Machine Learning},
	pages = {e1381},
	number = {6},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	author = {Gemaque, Rosana Noronha and Costa, Albert França Josuá and Giusti, Rafael and dos Santos, Eulanda Miranda},
	urldate = {2024-02-21},
	date = {2020},
	langid = {english},
	keywords = {classification, concept drift detectors, data streams, machine learning, nonstationary environments, online learning, overview},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\HQRUTGZ8\\Gemaque et al. - 2020 - An overview of unsupervised drift detection method.pdf:application/pdf;Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\MF49F7NL\\widm.html:text/html},
}

@article{oala_data_nodate,
	title = {Data Models for Dataset Drift Controls in Machine Learning With Optical Images},
	abstract = {Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important public services spanning medicine or environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of machine learning’s primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself to support the downstream machine vision task. This is an interesting upgrade to existing imaging pipelines which traditionally have been optimized to be consumed by human users but not machine learning models. The data models require access to raw sensor images as commonly processed at scale in industry domains such as microscopy, biomedicine, autonomous vehicles or remote sensing. Alongside the data model code we release two datasets to the public that we collected as part of this work. In total, the two datasets, Raw-Microscopy and Raw-Drone, comprise 1,488 scientifically calibrated reference raw sensor measurements, 8,928 raw intensity variations as well as 17,856 images processed through twelve data models with different configurations. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.},
	author = {Oala, Luis and Aversa, Marco and Nobis, Gabriel and Willis, Kurt and Neuenschwander, Yoan and Buck, Michèle and Matek, Christian and Extermann, Jérôme and Pomarico, Enrico and Samek, Wojciech and Murray-Smith, Roderick and Clausen, Christoph and Sanguinetti, Bruno},
	langid = {english},
	file = {Oala et al. - Data Models for Dataset Drift Controls in Machine .pdf:C\:\\Users\\wilkk\\Zotero\\storage\\SUZXTSCT\\Oala et al. - Data Models for Dataset Drift Controls in Machine .pdf:application/pdf},
}

@article{widmer_learning_1996,
	title = {Learning in the presence of concept drift and hidden contexts},
	volume = {23},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00116900},
	doi = {10.1007/BF00116900},
	abstract = {On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and reusing them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' perfomance under various conditions such as different levels of noise and different extent and rate of concept drift.},
	pages = {69--101},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	urldate = {2024-05-07},
	date = {1996-04-01},
	langid = {english},
	keywords = {concept drift, context dependence, forgetting, Incremental concept learning, on-line learning},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\TRS7PK6N\\Widmer and Kubat - 1996 - Learning in the presence of concept drift and hidd.pdf:application/pdf},
}

@article{kreuzberger_machine_2023,
	title = {Machine Learning Operations ({MLOps}): Overview, Definition, and Architecture},
	volume = {11},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/abstract/document/10081336},
	doi = {10.1109/ACCESS.2023.3262138},
	shorttitle = {Machine Learning Operations ({MLOps})},
	abstract = {The final goal of all industrial machine learning ({ML}) projects is to develop {ML} products and rapidly bring them into production. However, it is highly challenging to automate and operationalize {ML} products and thus many {ML} endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations ({MLOps}) addresses this issue. {MLOps} includes several aspects, such as best practices, sets of concepts, and development culture. However, {MLOps} is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of {MLOps} and highlight open challenges in the field. Finally, this work provides guidance for {ML} researchers and practitioners who want to automate and operate their {ML} products with a designated set of technologies.},
	pages = {31866--31879},
	journaltitle = {{IEEE} Access},
	author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
	urldate = {2024-05-10},
	date = {2023},
	note = {Conference Name: {IEEE} Access},
	keywords = {Automation, Bibliographies, {CI}/{CD}, Codes, Collaboration, {DevOps}, Interviews, machine learning, Machine learning, {MLOps}, operations, Training, workflow orchestration},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\wilkk\\Zotero\\storage\\AEBHWJ8I\\10081336.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\9W9NNVZ5\\Kreuzberger et al. - 2023 - Machine Learning Operations (MLOps) Overview, Def.pdf:application/pdf},
}

@article{young_empirical_2022,
	title = {Empirical evaluation of performance degradation of machine learning-based predictive models – A case study in healthcare information systems},
	volume = {2},
	issn = {2667-0968},
	url = {https://www.sciencedirect.com/science/article/pii/S2667096822000143},
	doi = {10.1016/j.jjimei.2022.100070},
	abstract = {While there have been a very large number of academic studies of proposed machine learning-based health predictive models, it is widely recognized that machine learning-based models in all domains typically degrade in performance over time, post training. This known characteristic of machine learning-based models could present significant risks in the healthcare setting to patient quality of care or safety. Nevertheless, there has been little study of the performance degradation of such models on real-world data. In this article, we empirically measure performance degradation of predictive models that predict at time of admission, emergency patient mortality, drawing upon a large dataset of over 1.83 million patient discharge records. We demonstrate important empirical results including both relatively slow performance degradation over two and a half years, but also significant differences in the rate and extent of performance degradation between different machine learning model types and time period of the training set.},
	pages = {100070},
	number = {1},
	journaltitle = {International Journal of Information Management Data Insights},
	shortjournal = {International Journal of Information Management Data Insights},
	author = {Young, Zachary and Steele, Robert},
	urldate = {2024-05-10},
	date = {2022-04-01},
	keywords = {Health information systems, Health predictive model, Machine learning, Performance degradation},
	file = {ScienceDirect Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\8X5GG8ZY\\S2667096822000143.html:text/html},
}

@misc{bayram_concept_2022,
	title = {From Concept Drift to Model Degradation: An Overview on Performance-Aware Drift Detectors},
	url = {http://arxiv.org/abs/2203.11070},
	shorttitle = {From Concept Drift to Model Degradation},
	abstract = {The dynamicity of real-world systems poses a significant challenge to deployed predictive machine learning ({ML}) models. Changes in the system on which the {ML} model has been trained may lead to performance degradation during the system's life cycle. Recent advances that study non-stationary environments have mainly focused on identifying and addressing such changes caused by a phenomenon called concept drift. Different terms have been used in the literature to refer to the same type of concept drift and the same term for various types. This lack of unified terminology is set out to create confusion on distinguishing between different concept drift variants. In this paper, we start by grouping concept drift types by their mathematical definitions and survey the different terms used in the literature to build a consolidated taxonomy of the field. We also review and classify performance-based concept drift detection methods proposed in the last decade. These methods utilize the predictive model's performance degradation to signal substantial changes in the systems. The classification is outlined in a hierarchical diagram to provide an orderly navigation between the methods. We present a comprehensive analysis of the main attributes and strategies for tracking and evaluating the model's performance in the predictive system. The paper concludes by discussing open research challenges and possible research directions.},
	number = {{arXiv}:2203.11070},
	publisher = {{arXiv}},
	author = {Bayram, Firas and Ahmed, Bestoun S. and Kassler, Andreas},
	urldate = {2024-05-10},
	date = {2022-03-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2203.11070 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Bayram et al. - 2022 - From Concept Drift to Model Degradation An Overvi.pdf:C\:\\Users\\wilkk\\Zotero\\storage\\ZQNXI2H4\\Bayram et al. - 2022 - From Concept Drift to Model Degradation An Overvi.pdf:application/pdf},
}

@misc{marcus_deep_2018,
	title = {Deep Learning: A Critical Appraisal},
	url = {http://arxiv.org/abs/1801.00631},
	doi = {10.48550/arXiv.1801.00631},
	shorttitle = {Deep Learning},
	abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
	number = {{arXiv}:1801.00631},
	publisher = {{arXiv}},
	author = {Marcus, Gary},
	urldate = {2024-05-10},
	date = {2018-01-02},
	eprinttype = {arxiv},
	eprint = {1801.00631 [cs, stat]},
	keywords = {97R40, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.0, I.2.6, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\wilkk\\Zotero\\storage\\VZHMS8L5\\Marcus - 2018 - Deep Learning A Critical Appraisal.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\8S4BYX4J\\1801.html:text/html},
}

@article{johnston_abstract_2023,
	title = {Abstract representations emerge naturally in neural networks trained to perform multiple tasks},
	volume = {14},
	rights = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-36583-0},
	doi = {10.1038/s41467-023-36583-0},
	abstract = {Humans and other animals demonstrate a remarkable ability to generalize knowledge across distinct contexts and objects during natural behavior. We posit that this ability to generalize arises from a specific representational geometry, that we call abstract and that is referred to as disentangled in machine learning. These abstract representations have been observed in recent neurophysiological studies. However, it is unknown how they emerge. Here, using feedforward neural networks, we demonstrate that the learning of multiple tasks causes abstract representations to emerge, using both supervised and reinforcement learning. We show that these abstract representations enable few-sample learning and reliable generalization on novel tasks. We conclude that abstract representations of sensory and cognitive variables may emerge from the multiple behaviors that animals exhibit in the natural world, and, as a consequence, could be pervasive in high-level brain regions. We also make several specific predictions about which variables will be represented abstractly.},
	pages = {1040},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Johnston, W. Jeffrey and Fusi, Stefano},
	urldate = {2024-05-10},
	date = {2023-02-23},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Learning algorithms, Neural encoding},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\Q4MSGDL8\\Johnston and Fusi - 2023 - Abstract representations emerge naturally in neura.pdf:application/pdf},
}

@article{hua_edge_2023,
	title = {Edge Computing with Artificial Intelligence: A Machine Learning Perspective},
	volume = {55},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3555802},
	doi = {10.1145/3555802},
	shorttitle = {Edge Computing with Artificial Intelligence},
	abstract = {Recent years have witnessed the widespread popularity of Internet of things ({IoT}). By providing sufficient data for model training and inference, {IoT} has promoted the development of artificial intelligence ({AI}) to a great extent. Under this background and trend, the traditional cloud computing model may nevertheless encounter many problems in independently tackling the massive data generated by {IoT} and meeting corresponding practical needs. In response, a new computing model called edge computing ({EC}) has drawn extensive attention from both industry and academia. With the continuous deepening of the research on {EC}, however, scholars have found that traditional (non-{AI}) methods have their limitations in enhancing the performance of {EC}. Seeing the successful application of {AI} in various fields, {EC} researchers start to set their sights on {AI}, especially from a perspective of machine learning, a branch of {AI} that has gained increased popularity in the past decades. In this article, we first explain the formal definition of {EC} and the reasons why {EC} has become a favorable computing model. Then, we discuss the problems of interest in {EC}. We summarize the traditional solutions and hightlight their limitations. By explaining the research results of using {AI} to optimize {EC} and applying {AI} to other fields under the {EC} architecture, this article can serve as a guide to explore new research ideas in these two aspects while enjoying the mutually beneficial relationship between {AI} and {EC}.},
	pages = {184:1--184:35},
	number = {9},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Hua, Haochen and Li, Yutong and Wang, Tonghe and Dong, Nanqing and Li, Wei and Cao, Junwei},
	urldate = {2024-05-12},
	date = {2023},
	keywords = {artificial intelligence, Edge computing, machine learning},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\PRCLZP28\\Hua et al. - 2023 - Edge Computing with Artificial Intelligence A Mac.pdf:application/pdf},
}

@article{merenda_edge_2020,
	title = {Edge Machine Learning for {AI}-Enabled {IoT} Devices: A Review},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/9/2533},
	doi = {10.3390/s20092533},
	shorttitle = {Edge Machine Learning for {AI}-Enabled {IoT} Devices},
	abstract = {In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors’ data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning “Hello World”.},
	pages = {2533},
	number = {9},
	journaltitle = {Sensors},
	author = {Merenda, Massimo and Porcaro, Carlo and Iero, Demetrio},
	urldate = {2024-05-12},
	date = {2020-01},
	langid = {english},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial intelligence, deep learning, edge devices, Internet of Things, machine learning},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\JH6AC5MD\\Merenda et al. - 2020 - Edge Machine Learning for AI-Enabled IoT Devices .pdf:application/pdf},
}

@misc{khouas_training_2024,
	title = {Training Machine Learning models at the Edge: A Survey},
	url = {http://arxiv.org/abs/2403.02619},
	doi = {10.48550/arXiv.2403.02619},
	shorttitle = {Training Machine Learning models at the Edge},
	abstract = {Edge Computing ({EC}) has gained significant traction in recent years, promising enhanced efficiency by integrating Artificial Intelligence ({AI}) capabilities at the edge. While the focus has primarily been on the deployment and inference of Machine Learning ({ML}) models at the edge, the training aspect remains less explored. This survey delves into Edge Learning ({EL}), specifically the optimization of {ML} model training at the edge. The objective is to comprehensively explore diverse approaches and methodologies in {EL}, synthesize existing knowledge, identify challenges, and highlight future trends. Utilizing Scopus' advanced search, relevant literature on {EL} was identified, revealing a concentration of research efforts in distributed learning methods, particularly Federated Learning ({FL}). This survey further provides a guideline for comparing techniques used to optimize {ML} for edge learning, along with an exploration of different frameworks, libraries, and simulation tools available for {EL}. In doing so, the paper contributes to a holistic understanding of the current landscape and future directions in the intersection of edge computing and machine learning, paving the way for informed comparisons between optimization methods and techniques designed for edge learning.},
	number = {{arXiv}:2403.02619},
	publisher = {{arXiv}},
	author = {Khouas, Aymen Rayane and Bouadjenek, Mohamed Reda and Hacid, Hakim and Aryal, Sunil},
	urldate = {2024-05-12},
	date = {2024-03-13},
	eprinttype = {arxiv},
	eprint = {2403.02619 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\wilkk\\Zotero\\storage\\95AZEBDM\\Khouas et al. - 2024 - Training Machine Learning models at the Edge A Su.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\7S5FSHIS\\2403.html:text/html},
}

@article{murshed_machine_2021,
	title = {Machine Learning at the Network Edge: A Survey},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3469029},
	doi = {10.1145/3469029},
	shorttitle = {Machine Learning at the Network Edge},
	abstract = {Resource-constrained {IoT} devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for {AI} systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e., close to the {IoT} devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.},
	pages = {170:1--170:37},
	number = {8},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Murshed, M. G. Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz},
	urldate = {2024-05-12},
	date = {2021},
	keywords = {deep learning, distributed computing, Edge intelligence, embedded, {IoT}, low-power, machine learning, mobile edge computing, resource-constrained},
	file = {Submitted Version:C\:\\Users\\wilkk\\Zotero\\storage\\CWL3SCVY\\Murshed et al. - 2021 - Machine Learning at the Network Edge A Survey.pdf:application/pdf},
}

@article{lu_learning_2018,
	title = {Learning under Concept Drift: A Review},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {https://ieeexplore.ieee.org/document/8496795/},
	doi = {10.1109/TKDE.2018.2876857},
	shorttitle = {Learning under Concept Drift},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
	urldate = {2024-05-13},
	date = {2018},
	file = {Submitted Version:C\:\\Users\\wilkk\\Zotero\\storage\\FUAZA3MK\\Lu et al. - 2018 - Learning under Concept Drift A Review.pdf:application/pdf},
}

@misc{soin_chexstray_2022,
	title = {{CheXstray}: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging {AI}},
	url = {http://arxiv.org/abs/2202.02833},
	shorttitle = {{CheXstray}},
	abstract = {Clinical Artificial lntelligence ({AI}) applications are rapidly expanding worldwide, and have the potential to impact to all areas of medical practice. Medical imaging applications constitute a vast majority of approved clinical {AI} applications. Though healthcare systems are eager to adopt {AI} solutions a fundamental question remains: {\textbackslash}textit\{what happens after the {AI} model goes into production?\} We use the {CheXpert} and {PadChest} public datasets to build and test a medical imaging {AI} drift monitoring workflow to track data and model drift without contemporaneous ground truth. We simulate drift in multiple experiments to compare model performance with our novel multi-modal drift metric, which uses {DICOM} metadata, image appearance representation from a variational autoencoder ({VAE}), and model output probabilities as input. Through experimentation, we demonstrate a strong proxy for ground truth performance using unsupervised distributional shifts in relevant metadata, predicted probabilities, and {VAE} latent representation. Our key contributions include (1) proof-of-concept for medical imaging drift detection that includes the use of {VAE} and domain specific statistical methods, (2) a multi-modal methodology to measure and unify drift metrics, (3) new insights into the challenges and solutions to observe deployed medical imaging {AI}, and (4) creation of open-source tools that enable others to easily run their own workflows and scenarios. This work has important implications. It addresses the concerning translation gap found in continuous medical imaging {AI} model monitoring common in dynamic healthcare environments.},
	number = {{arXiv}:2202.02833},
	publisher = {{arXiv}},
	author = {Soin, Arjun and Merkow, Jameson and Long, Jin and Cohen, Joseph Paul and Saligrama, Smitha and Kaiser, Stephen and Borg, Steven and Tarapov, Ivan and Lungren, Matthew P.},
	urldate = {2024-05-13},
	date = {2022-03-17},
	eprinttype = {arxiv},
	eprint = {2202.02833 [cs, eess]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\PHXMLXLF\\2202.html:text/html;Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\WWUVWBQS\\Soin et al. - 2022 - CheXstray Real-time Multi-Modal Data Concordance .pdf:application/pdf},
}

@article{barros_large-scale_2018,
	title = {A large-scale comparison of concept drift detectors},
	volume = {451-452},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518302743},
	doi = {10.1016/j.ins.2018.04.014},
	abstract = {Online learning involves extracting information from large quantities of data (streams) usually affected by changes in the distribution (concept drift). A drift detector is a small program that estimates the positions of these changes to replace the base learner and ultimately improve overall accuracy. This article reports on a large-scale comparison of 14 concept drift detector configurations for mining fully labeled data streams with concept drift, using a large number of artificial datasets and two different base classifiers (Naive Bayes and Hoeffding Tree). The goal is to adequately measure how good the existent concept drift detectors really are and also to verify and challenge a common belief in the area, namely that the best drift detection methods are necessarily those that detect all the existing drifts closer to their correct positions, and only them, irrespective of the fact that different objectives usually require alternative solutions. Finally, to some extent, this article may also be seen as an extensive literature survey of concept drift detectors.},
	pages = {348--370},
	journaltitle = {Information Sciences},
	shortjournal = {Information Sciences},
	author = {Barros, Roberto Souto Maior and Santos, Silas Garrido T. Carvalho},
	urldate = {2024-05-13},
	date = {2018-07-01},
	keywords = {Concept drift, Data stream, Drift detection, Large-scale comparison, Online learning},
}

@article{krawczyk_ensemble_2017,
	title = {Ensemble learning for data stream analysis: A survey},
	volume = {37},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253516302329},
	doi = {10.1016/j.inffus.2017.02.004},
	shorttitle = {Ensemble learning for data stream analysis},
	abstract = {In many applications of information systems learning algorithms have to act in dynamic environments where data are collected in the form of transient data streams. Compared to static data mining, processing streams imposes new computational requirements for algorithms to incrementally process incoming examples while using limited memory and time. Furthermore, due to the non-stationary characteristics of streaming data, prediction models are often also required to adapt to concept drifts. Out of several new proposed stream algorithms, ensembles play an important role, in particular for non-stationary environments. This paper surveys research on ensembles for data stream classification as well as regression tasks. Besides presenting a comprehensive spectrum of ensemble approaches for data streams, we also discuss advanced learning concepts such as imbalanced data streams, novelty detection, active and semi-supervised learning, complex data representations and structured outputs. The paper concludes with a discussion of open research problems and lines of future research.},
	pages = {132--156},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Krawczyk, Bartosz and Minku, Leandro L. and Gama, João and Stefanowski, Jerzy and Woźniak, Michał},
	urldate = {2024-05-13},
	date = {2017-09-01},
	keywords = {Concept drift, Data streams, Ensemble learning, Non-stationary environments, Online learning},
	file = {Full Text:C\:\\Users\\wilkk\\Zotero\\storage\\EA9B5PL8\\Krawczyk et al. - 2017 - Ensemble learning for data stream analysis A surv.pdf:application/pdf},
}

@article{frias-blanco_online_2015,
	title = {Online and Non-Parametric Drift Detection Methods Based on Hoeffding’s Bounds},
	volume = {27},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/6871418},
	doi = {10.1109/TKDE.2014.2345382},
	abstract = {Incremental and online learning algorithms are more relevant in the data mining context because of the increasing necessity to process data streams. In this context, the target function may change overtime, an inherent problem of online learning (known as concept drift). In order to handle concept drift regardless of the learning model, we propose new methods to monitor the performance metrics measured during the learning process, to trigger drift signals when a significant variation has been detected. To monitor this performance, we apply some probability inequalities that assume only independent, univariate and bounded random variables to obtain theoretical guarantees for the detection of such distributional changes. Some common restrictions for the online change detection as well as relevant types of change (abrupt and gradual) are considered. Two main approaches are proposed, the first one involves moving averages and is more suitable to detect abrupt changes. The second one follows a widespread intuitive idea to deal with gradual changes using weighted moving averages. The simplicity of the proposed methods, together with the computational efficiency make them very advantageous. We use a Naive Bayes classifier and a Perceptron to evaluate the performance of the methods over synthetic and real data.},
	pages = {810--823},
	number = {3},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Frías-Blanco, Isvani and Campo-Ávila, José del and Ramos-Jiménez, Gonzalo and Morales-Bueno, Rafael and Ortiz-Díaz, Agustín and Caballero-Mota, Yailé},
	urldate = {2024-05-13},
	date = {2015-03},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Concept drift, control chart, Data models, Detectors, Electronic mail, incremental learning, Monitoring, Random variables, Vectors, weighted moving average},
	file = {Accepted Version:C\:\\Users\\wilkk\\Zotero\\storage\\X9J3K7H8\\Frías-Blanco et al. - 2015 - Online and Non-Parametric Drift Detection Methods .pdf:application/pdf},
}

@article{ghosh_edge-cloud_2021,
	title = {Edge-Cloud Computing for Internet of Things Data Analytics: Embedding Intelligence in the Edge With Deep Learning},
	volume = {17},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/abstract/document/9139356},
	doi = {10.1109/TII.2020.3008711},
	shorttitle = {Edge-Cloud Computing for Internet of Things Data Analytics},
	abstract = {Rapid growth in numbers of connected devices including sensors, mobile, wearable, and other Internet of Things ({IoT}) devices, is creating an explosion of data that are moving across the network. To carry out machine learning ({ML}), {IoT} data are typically transferred to the cloud or another centralized system for storage and processing; however, this causes latencies and increases network traffic. Edge computing has the potential to remedy those issues by moving computation closer to the network edge and data sources. On the other hand, edge computing is limited in terms of computational power, and thus, is not well-suited for {ML} tasks. Consequently, this article aims to combine edge and cloud computing for {IoT} data analytics by taking advantage of edge nodes to reduce data transfer. In order to process data close to the source, sensors are grouped according to locations, and feature learning is performed on the close by edge node. For comparison reasons, similarity-based processing is also considered. Feature learning is carried out with deep learning - the encoder part of the trained autoencoder is placed on the edge and the decoder part is placed on the cloud. The evaluation was performed on the task of human activity recognition from sensor data. The results show that when sliding windows are used in the preparation step, data can be reduced on the edge up to 80\% without significant loss in accuracy.},
	pages = {2191--2200},
	number = {3},
	journaltitle = {{IEEE} Transactions on Industrial Informatics},
	author = {Ghosh, Ananda Mohon and Grolinger, Katarina},
	urldate = {2024-05-13},
	date = {2021-03},
	note = {Conference Name: {IEEE} Transactions on Industrial Informatics},
	keywords = {Autoencoders ({AEs}), Cloud computing, Computational modeling, Data analysis, data reduction, deep learning ({DL}), Edge computing, edge computing ({EC}), human activity recognition ({HAR}), Internet of Things, Internet of Things ({IoT}), Sensors, Task analysis},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\VC9QJBYX\\Ghosh and Grolinger - 2021 - Edge-Cloud Computing for Internet of Things Data A.pdf:application/pdf},
}

@article{kore_empirical_2024,
	title = {Empirical data drift detection experiments on real-world medical imaging data},
	volume = {15},
	rights = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-46142-w},
	doi = {10.1038/s41467-024-46142-w},
	abstract = {While it is common to monitor deployed clinical artificial intelligence ({AI}) models for performance degradation, it is less common for the input data to be monitored for data drift – systemic changes to input distributions. However, when real-time evaluation may not be practical (eg., labeling costs) or when gold-labels are automatically generated, we argue that tracking data drift becomes a vital addition for {AI} deployments. In this work, we perform empirical experiments on real-world medical imaging to evaluate three data drift detection methods’ ability to detect data drift caused (a) naturally (emergence of {COVID}-19 in X-rays) and (b) synthetically. We find that monitoring performance alone is not a good proxy for detecting data drift and that drift-detection heavily depends on sample size and patient features. Our work discusses the need and utility of data drift detection in various scenarios and highlights gaps in knowledge for the practical application of existing methods.},
	pages = {1887},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Kore, Ali and Abbasi Bavil, Elyar and Subasri, Vallijah and Abdalla, Moustafa and Fine, Benjamin and Dolatabadi, Elham and Abdalla, Mohamed},
	urldate = {2024-05-13},
	date = {2024-02-29},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational models, Computational science, Medical imaging},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\N6AV24EJ\\Kore et al. - 2024 - Empirical data drift detection experiments on real.pdf:application/pdf},
}

@article{barros_overview_2019,
	title = {An overview and comprehensive comparison of ensembles for concept drift},
	volume = {52},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253518308066},
	doi = {10.1016/j.inffus.2019.03.006},
	abstract = {Online learning is about extracting information from large data streams which may be affected by changes in the distribution of the data, events known as concept drift. Concept drift detectors are small programs that try to detect these changes and make it possible to replace the base classifier, improving the overall accuracy. Ensembles of classifiers are also common in this application area and some of them are configurable with a drift detector. This article summarizes a large-scale comparison of six ensemble algorithms, configured with 10 different drift detectors, for learning from fully labeled data streams, using a large number of artificial datasets and two popular base learners in the area: Naive Bayes and Hoeffding Tree. In addition, the code of one the ensembles (Leveraging Bagging) was modified to permit its configuration with any drift detector: its original implementation only uses {ADWIN}. The goal is to assess how good the existing ensemble algorithms configurable with detectors really are and also to verify and challenge a common belief in the area. The results of the experiments suggest that, in most datasets, the choice of ensemble algorithm has much more impact on the final accuracy than the choice of drift detector used in its configuration. They also suggest the best auxiliary detectors to configure the ensembles, i.e. those that maximize the accuracy of the ensembles, are only marginally different from the best detectors in the same datasets in terms of their accuracies (recently reported in another article).},
	pages = {213--244},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Barros, Roberto Souto Maior de and Santos, Silas Garrido T. de Carvalho},
	urldate = {2024-05-13},
	date = {2019-12-01},
	keywords = {Concept drift, Data stream, Detectors, Ensembles, Large-scale comparison, Online learning},
	file = {ScienceDirect Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\X4AIHFL4\\S1566253518308066.html:text/html},
}

@article{iwashita_overview_2019,
	title = {An Overview on Concept Drift Learning},
	volume = {7},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/8571222},
	doi = {10.1109/ACCESS.2018.2886026},
	abstract = {Concept drift techniques aim at learning patterns from data streams that may change over time. Although such behavior is not usually expected in controlled environments, real-world scenarios can face changes in the data, such as new classes, clusters, and features. Traditional classifiers can be easily fooled in such situations, resulting in poor performances. Common concept drift domains include recommendation systems, energy consumption, artificial intelligence systems with dynamic environment interaction, and biomedical signal analysis (e.g., neurogenerative diseases). In this paper, we surveyed several works that deal with concept drift, as well as we presented a comprehensive study of public synthetic and real datasets that can be used to cope with such a problem. In addition, we considered a review of different types of drifts and approaches to handling such changes in the data. We considered different learners employed in classification tasks and the use of drift detection mechanisms, among other characteristics.},
	pages = {1532--1547},
	journaltitle = {{IEEE} Access},
	author = {Iwashita, Adriana Sayuri and Papa, João Paulo},
	urldate = {2024-05-13},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	keywords = {Bayes methods, Concept drift, Finance, Heuristic algorithms, machine learning, Memory management, Noise measurement, pattern recognition, Support vector machines, Training},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\wilkk\\Zotero\\storage\\R8P5TXRF\\8571222.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\2I6DSNQL\\Iwashita and Papa - 2019 - An Overview on Concept Drift Learning.pdf:application/pdf},
}

@inproceedings{cavalcante_fedd_2016,
	title = {{FEDD}: Feature Extraction for Explicit Concept Drift Detection in time series},
	doi = {10.1109/IJCNN.2016.7727274},
	shorttitle = {{FEDD}},
	pages = {740--747},
	author = {Cavalcante, Rodolfo and Minku, Leandro and Oliveira, Adriano},
	date = {2016-07-01},
	file = {Full Text:C\:\\Users\\wilkk\\Zotero\\storage\\8FAKSCQL\\Cavalcante et al. - 2016 - FEDD Feature Extraction for Explicit Concept Drif.pdf:application/pdf},
}

@article{baena-garcia_early_2006,
	title = {Early Drift Detection Method},
	abstract = {An emerging problem in Data Streams is the detection of concept drift. This problem is aggravated when the drift is gradual over time. In this work we deflne a method for detecting concept drift, even in the case of slow gradual change. It is based on the estimated distribution of the distances between classiflcation errors. The proposed method can be used with any learning algorithm in two ways: using it as a wrapper of a batch learning algorithm or implementing it inside an incremental and online algorithm. The experimentation results compare our method ({EDDM}) with a similar one ({DDM}). Latter uses the error-rate instead of distance-error-rate.},
	author = {Baena-García, Manuel and Campo-Ávila, José and Fidalgo-Merino, Raúl and Bifet, Albert and Gavald, Ricard and Morales-Bueno, Rafael},
	date = {2006-01-01},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\F2SAMPVC\\Baena-García et al. - 2006 - Early Drift Detection Method.pdf:application/pdf},
}

@article{goncalves_concept_2024,
	title = {Concept drift adaptation in video surveillance: a systematic review},
	volume = {83},
	issn = {1573-7721},
	url = {https://doi.org/10.1007/s11042-023-15855-3},
	doi = {10.1007/s11042-023-15855-3},
	shorttitle = {Concept drift adaptation in video surveillance},
	abstract = {The world we live in is dynamic by nature. Frequently, the environment changes in ways we cannot predict. In machine learning, the phenomenon that occurs when a model has its prediction effectiveness degraded due to unforeseen changes is known as concept drift. Applications of smart video surveillance tend to suffer from concept drift due to changes in illumination, weather, and scene structure. This work differs from previous ones as it brings focus to the problem of concept drift from a surveillance video perspective which presents additional challenges compared to other sources of data, such as high dimensionality, spatial and temporal relations between data, and real-time constraints. The approaches and algorithms used to cope with concept drift are compared and discussed. We also present datasets and metrics used to evaluate the effectiveness of the algorithms.},
	pages = {9997--10037},
	number = {4},
	journaltitle = {Multimedia Tools and Applications},
	shortjournal = {Multimed Tools Appl},
	author = {Goncalves, Vinicius P. M. and Silva, Lourival P. and Nunes, Fatima L. S. and Ferreira, João E. and Araújo, Luciano V.},
	urldate = {2024-05-13},
	date = {2024-01-01},
	langid = {english},
	keywords = {Adaptation, Change detection, Concept drift, Surveillance, Video},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\UY9FMVK2\\Goncalves et al. - 2024 - Concept drift adaptation in video surveillance a .pdf:application/pdf},
}

@misc{suprem_odin_2020,
	title = {{ODIN}: Automated Drift Detection and Recovery in Video Analytics},
	url = {http://arxiv.org/abs/2009.05440},
	doi = {10.48550/arXiv.2009.05440},
	shorttitle = {{ODIN}},
	abstract = {Recent advances in computer vision have led to a resurgence of interest in visual data analytics. Researchers are developing systems for effectively and efficiently analyzing visual data at scale. A significant challenge that these systems encounter lies in the drift in real-world visual data. For instance, a model for self-driving vehicles that is not trained on images containing snow does not work well when it encounters them in practice. This drift phenomenon limits the accuracy of models employed for visual data analytics. In this paper, we present a visual data analytics system, called {ODIN}, that automatically detects and recovers from drift. {ODIN} uses adversarial autoencoders to learn the distribution of high-dimensional images. We present an unsupervised algorithm for detecting drift by comparing the distributions of the given data against that of previously seen data. When {ODIN} detects drift, it invokes a drift recovery algorithm to deploy specialized models tailored towards the novel data points. These specialized models outperform their non-specialized counterpart on accuracy, performance, and memory footprint. Lastly, we present a model selection algorithm for picking an ensemble of best-fit specialized models to process a given input. We evaluate the efficacy and efficiency of {ODIN} on high-resolution dashboard camera videos captured under diverse environments from the Berkeley {DeepDrive} dataset. We demonstrate that {ODIN}'s models deliver 6x higher throughput, 2x higher accuracy, and 6x smaller memory footprint compared to a baseline system without automated drift detection and recovery.},
	number = {{arXiv}:2009.05440},
	publisher = {{arXiv}},
	author = {Suprem, Abhijit and Arulraj, Joy and Pu, Calton and Ferreira, Joao},
	urldate = {2024-05-13},
	date = {2020-09-09},
	eprinttype = {arxiv},
	eprint = {2009.05440 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\wilkk\\Zotero\\storage\\B8EA9R9D\\Suprem et al. - 2020 - ODIN Automated Drift Detection and Recovery in Vi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\YQ87PYVB\\2009.html:text/html},
}

@misc{oala_data_2023,
	title = {Data Models for Dataset Drift Controls in Machine Learning With Optical Images},
	url = {http://arxiv.org/abs/2211.02578},
	doi = {10.48550/arXiv.2211.02578},
	abstract = {Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.},
	number = {{arXiv}:2211.02578},
	publisher = {{arXiv}},
	author = {Oala, Luis and Aversa, Marco and Nobis, Gabriel and Willis, Kurt and Neuenschwander, Yoan and Buck, Michèle and Matek, Christian and Extermann, Jerome and Pomarico, Enrico and Samek, Wojciech and Murray-Smith, Roderick and Clausen, Christoph and Sanguinetti, Bruno},
	urldate = {2024-05-13},
	date = {2023-05-07},
	eprinttype = {arxiv},
	eprint = {2211.02578 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\wilkk\\Zotero\\storage\\MMZ4ZVCJ\\Oala et al. - 2023 - Data Models for Dataset Drift Controls in Machine .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\CWLGWV7Y\\2211.html:text/html},
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2024-05-15},
	date = {2019-07-06},
	keywords = {Big data, Data Augmentation, Deep Learning, {GANs}, Image data},
	file = {Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\SBDV97BY\\Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf:application/pdf},
}

@inproceedings{costa_drift_2018,
	title = {A Drift Detection Method Based on Active Learning},
	url = {https://ieeexplore.ieee.org/document/8489364},
	doi = {10.1109/IJCNN.2018.8489364},
	abstract = {Several real-world prediction problems are subject to changes over time due to their dynamic nature. These changes, named concept drift, usually lead to immediate and disastrous loss in classifier's performance. In order to cope with such a serious problem, drift detection methods have been proposed in the literature. However, current methods cannot be widely used since they are based either on performance monitoring or on fully labeled data, or even both. Focusing on overcoming these drawbacks, in this work we propose using density variation of the most significant instances as an explicit unsupervised trigger for concept drift detection. Here, density variation is based on Active Learning, and it is calculated from virtual margins projected onto the input space according to classifier confidence. In order to investigate the performance of the proposed method, we have carried out experiments on six databases, precisely four synthetic and two real databases focusing on setting up all parameters involved in our method and on comparing it to three baselines, including two supervised drift detectors and one Active Learning-based strategy. The obtained results show that our method, when compared to the supervised baselines, reached better recognition rates in the majority of the investigated databases, while keeping similar or higher detection rates. In terms of the Active Learning-based strategies comparison, our method outperformed the baseline taking into account both recognition and detection rates, even though the baseline employed much less labeled samples. Therefore, the proposed method established a better trade-off between amount of labeled samples and detection capability, as well as recognition rate.},
	eventtitle = {2018 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2018 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Costa, Albert França Josuá and Albuquerque, Régis Ant{\textbackslash}\&nio Saraiva and Santos, Eulanda Miranda dos},
	urldate = {2024-05-15},
	date = {2018-07},
	note = {{ISSN}: 2161-4407},
	keywords = {active learning, concept drift detection, Databases, Detectors, Error analysis, Focusing, Labeling, Monitoring, uncertainty, Uncertainty, virtual margins},
}

@inproceedings{dos_reis_fast_2016,
	location = {New York, {NY}, {USA}},
	title = {Fast Unsupervised Online Drift Detection Using Incremental Kolmogorov-Smirnov Test},
	isbn = {978-1-4503-4232-2},
	url = {https://doi.org/10.1145/2939672.2939836},
	doi = {10.1145/2939672.2939836},
	series = {{KDD} '16},
	abstract = {Data stream research has grown rapidly over the last decade. Two major features distinguish data stream from batch learning: stream data are generated on the fly, possibly in a fast and variable rate; and the underlying data distribution can be non-stationary, leading to a phenomenon known as concept drift. Therefore, most of the research on data stream classification focuses on proposing efficient models that can adapt to concept drifts and maintain a stable performance over time. However, specifically for the classification task, the majority of such methods rely on the instantaneous availability of true labels for all already classified instances. This is a strong assumption that is rarely fulfilled in practical applications. Hence there is a clear need for efficient methods that can detect concept drifts in an unsupervised way. One possibility is the well-known Kolmogorov-Smirnov test, a statistical hypothesis test that checks whether two samples differ. This work has two main contributions. The first one is the Incremental Kolmogorov-Smirnov algorithm that allows performing the Kolmogorov-Smirnov hypothesis test instantly using two samples that change over time, where the change is an insertion and/or removal of an observation. Our algorithm employs a randomized tree and is able to perform the insertion and removal operations in O(log N) with high probability and calculate the Kolmogorov-Smirnov test in O(1), where N is the number of sample observations. This is a significant speed-up compared to the O(N log N) cost of the non-incremental implementation. The second contribution is the use of the Incremental Kolmogorov-Smirnov test to detect concept drifts without true labels. Classification algorithms adapted to use the test rely on a limited portion of those labels just to update the classification model after a concept drift is detected.},
	pages = {1545--1554},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {dos Reis, Denis Moreira and Flach, Peter and Matwin, Stan and Batista, Gustavo},
	urldate = {2024-05-14},
	date = {2016},
	keywords = {cartesian tree, concept drift, data stream, kolmogorov-smirnov, lazy propagation, treap},
	file = {Submitted Version:C\:\\Users\\wilkk\\Zotero\\storage\\M6JRWM5D\\dos Reis et al. - 2016 - Fast Unsupervised Online Drift Detection Using Inc.pdf:application/pdf},
}

@online{noauthor_learning_nodate,
	title = {Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in {VHR} Optical Remote Sensing Images {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/abstract/document/7560644},
	urldate = {2024-05-15},
	file = {Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images | IEEE Journals & Magazine | IEEE Xplore:C\:\\Users\\wilkk\\Zotero\\storage\\S8VC2IBJ\\7560644.html:text/html},
}

@article{cheng_learning_2016,
	title = {Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in {VHR} Optical Remote Sensing Images},
	volume = {54},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {0196-2892, 1558-0644},
	url = {http://ieeexplore.ieee.org/document/7560644/},
	doi = {10.1109/TGRS.2016.2601622},
	pages = {7405--7415},
	number = {12},
	journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
	shortjournal = {{IEEE} Trans. Geosci. Remote Sensing},
	author = {Cheng, Gong and Zhou, Peicheng and Han, Junwei},
	urldate = {2024-05-15},
	date = {2016-12},
}


@online{noauthor_cifar-10_nodate,
	title = {{CIFAR}-10: Image Classification {CNN} (89\%)},
	url = {https://kaggle.com/code/sachinpatil1280/cifar-10-image-classification-cnn-89},
	shorttitle = {{CIFAR}-10},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from {CIFAR}-10 - Object Recognition in Images},
	urldate = {2024-08-15},
	langid = {english},
	file = {Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\FHZLQKLM\\cifar-10-image-classification-cnn-89.html:text/html},
}


@online{noauthor_adam_nodate,
	title = {Adam — {PyTorch} 2.4 documentation},
	url = {https://pytorch.org/docs/stable/generated/torch.optim.Adam.html},
	urldate = {2024-08-15},
	file = {Adam — PyTorch 2.4 documentation:C\:\\Users\\wilkk\\Zotero\\storage\\MTPMMNAC\\torch.optim.Adam.html:text/html},
}


@misc{wang_ordisco_2021,
	title = {{ORDisCo}: Effective and Efficient Usage of Incremental Unlabeled Data for Semi-supervised Continual Learning},
	url = {http://arxiv.org/abs/2101.00407},
	shorttitle = {{ORDisCo}},
	abstract = {Continual learning usually assumes the incoming data are fully labeled, which might not be applicable in real applications. In this work, we consider semi-supervised continual learning ({SSCL}) that incrementally learns from partially labeled data. Observing that existing continual learning methods lack the ability to continually exploit the unlabeled data, we propose deep Online Replay with Discriminator Consistency ({ORDisCo}) to interdependently learn a classifier with a conditional generative adversarial network ({GAN}), which continually passes the learned data distribution to the classifier. In particular, {ORDisCo} replays data sampled from the conditional generator to the classifier in an online manner, exploiting unlabeled data in a time- and storage-efficient way. Further, to explicitly overcome the catastrophic forgetting of unlabeled data, we selectively stabilize parameters of the discriminator that are important for discriminating the pairs of old unlabeled data and their pseudo-labels predicted by the classifier. We extensively evaluate {ORDisCo} on various semi-supervised learning benchmark datasets for {SSCL}, and show that {ORDisCo} achieves significant performance improvement on {SVHN}, {CIFAR}10 and Tiny-{ImageNet}, compared to strong baselines.},
	number = {{arXiv}:2101.00407},
	publisher = {{arXiv}},
	author = {Wang, Liyuan and Yang, Kuo and Li, Chongxuan and Hong, Lanqing and Li, Zhenguo and Zhu, Jun},
	urldate = {2024-01-28},
	date = {2021-04-08},
	eprinttype = {arxiv},
	eprint = {2101.00407 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\wilkk\\Zotero\\storage\\CY8HHVLB\\2101.html:text/html;Full Text PDF:C\:\\Users\\wilkk\\Zotero\\storage\\ANHTWQPX\\Wang et al. - 2021 - ORDisCo Effective and Efficient Usage of Incremen.pdf:application/pdf},
}
