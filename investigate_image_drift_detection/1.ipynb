{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m      4\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      5\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mRandomResizedCrop(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m1.0\u001b[39m), ratio\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m1.3333\u001b[39m)),\n\u001b[0;32m      6\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m      7\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[0;32m      8\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      9\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResize(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m),\n\u001b[0;32m     10\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCenterCrop(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m     11\u001b[0m     torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor()])\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mOurDataModule\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLightningDataModule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOurDataModule\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m, in \u001b[0;36mOurDataModule\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOurDataModule\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent: \u001b[43mOptional\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOurDataModule\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, additional_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/hymenoptera_data/train/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m                                                                   transform\u001b[38;5;241m=\u001b[39mtrain_transform)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "## torchvision.datasets.utils.download_and_extract_archive('https://download.pytorch.org/tutorial/hymenoptera_data.zip', 'data/')\n",
    "\n",
    "# these are the standard transforms without the normalization (which we move into the model.step/predict before the forward)\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "val_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=256),\n",
    "    torchvision.transforms.CenterCrop(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "class OurDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, parent: Optional['OurDataModule']=None, additional_transform=None):\n",
    "        if parent is None:\n",
    "            self.train_dataset = torchvision.datasets.ImageFolder('./data/hymenoptera_data/train/',\n",
    "                                                                  transform=train_transform)\n",
    "            self.val_dataset = torchvision.datasets.ImageFolder('./data/hymenoptera_data/val/',\n",
    "                                                                  transform=val_transform)\n",
    "            self.test_dataset = torchvision.datasets.ImageFolder('./data/hymenoptera_data/test/',\n",
    "                                                                  transform=val_transform)\n",
    "            self.train_batch_size = 4\n",
    "            self.val_batch_size = 128\n",
    "            self.additional_transform = None\n",
    "        else:\n",
    "            self.train_dataset = parent.train_dataset\n",
    "            self.val_dataset = parent.val_dataset\n",
    "            self.test_dataset = parent.test_dataset\n",
    "            self.train_batch_size = parent.train_batch_size\n",
    "            self.val_batch_size = parent.val_batch_size\n",
    "            self.additional_transform = additional_transform\n",
    "        if additional_transform is not None:\n",
    "            self.additional_transform = additional_transform\n",
    "\n",
    "        self.prepare_data()\n",
    "        self.setup('fit')\n",
    "        self.setup('test')\n",
    "\n",
    "    def setup(self, typ):\n",
    "        pass\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        batch = torch.utils.data._utils.collate.default_collate(batch)\n",
    "        if self.additional_transform:\n",
    "            batch = (self.additional_transform(batch[0]), *batch[1:])\n",
    "        return batch\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.train_batch_size,\n",
    "                                           num_workers=4, shuffle=True, collate_fn=self.collate_fn)\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.val_batch_size,\n",
    "                                           num_workers=4, shuffle=False, collate_fn=self.collate_fn)\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.val_batch_size,\n",
    "                                           num_workers=4, shuffle=False, collate_fn=self.collate_fn)\n",
    "\n",
    "    def default_dataloader(self, batch_size=None, num_samples=None, shuffle=True):\n",
    "        dataset = self.val_dataset\n",
    "        if batch_size is None:\n",
    "            batch_size = self.val_batch_size\n",
    "        replacement = num_samples is not None\n",
    "        if shuffle:\n",
    "            sampler = torch.utils.data.RandomSampler(dataset, replacement=replacement, num_samples=num_samples)\n",
    "        else:\n",
    "            sampler = None\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                                           collate_fn=self.collate_fn)\n",
    "\n",
    "\n",
    "datamodule = OurDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Drift_image_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
